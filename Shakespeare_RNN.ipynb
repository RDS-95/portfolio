{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b532030a",
   "metadata": {},
   "source": [
    "# Natural Language Processing Using RNNs and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a63f9fd7",
   "metadata": {},
   "source": [
    "#### Generating Shakespeare using a character RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "baeb34fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "97037062",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_url = 'https://homl.info/shakespeare'\n",
    "filepath = keras.utils.get_file('shakespeare.txt', shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce0c3c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now encode every character as an integer\n",
    "\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(char_level=True)\n",
    "tokenizer.fit_on_texts([shakespeare_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47459611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[20, 6, 9, 8, 3]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.texts_to_sequences(['First'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "776d81d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['f i r s t']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.sequences_to_texts([[20, 6, 9, 8, 3]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b658bd4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_id = len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e67a335",
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode the full text so each character is represented by its ID\n",
    "\n",
    "[encoded] = np.array(tokenizer.texts_to_sequences([shakespeare_text])) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3e149ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the data set needs to be done carefully, since it is sequential\n",
    "\n",
    "dataset_size = encoded.shape[0]\n",
    "train_size = dataset_size * 90//100\n",
    "dataset = tf.data.Dataset.from_tensor_slices(encoded[:train_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b228fc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we need to use the data sets window method to convert the long string of characters to smaller readable pieces.\n",
    "\n",
    "n_steps = 100\n",
    "window_length = n_steps + 1\n",
    "dataset = dataset.window(window_length, shift=1, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1531c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#flatten the data set for training\n",
    "\n",
    "dataset = dataset.flat_map(lambda window: window.batch(window_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e500840",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we can shuffle the windows \n",
    "\n",
    "batch_size = 32\n",
    "dataset = dataset.shuffle(10000).batch(batch_size)\n",
    "dataset = dataset.map(lambda windows: (windows[:, :-1], windows[:, 1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f44d3158",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we only havee 39 distinct characters so lets encode them as one hot vectors\n",
    "\n",
    "dataset = dataset.map(\n",
    "    lambda X_batch, Y_batch: (tf.one_hot(X_batch, depth=max_id), Y_batch))\n",
    "\n",
    "dataset = dataset.prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5a9cc3ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 100, 39) (32, 100)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, Y_batch in dataset.take(1):\n",
    "    print(X_batch.shape, Y_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bfe3a84d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now the data has been prepared and we can move on to creating the model.\n",
    "\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.GRU(128, return_sequences=True, input_shape=[None,max_id],\n",
    "                    dropout=0.2),\n",
    "    keras.layers.GRU(128, return_sequences=True,\n",
    "                    dropout=0.2),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(max_id, activation='softmax'))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "efb0bd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "31368/31368 [==============================] - 11527s 367ms/step - loss: 1.6202\n",
      "Epoch 2/20\n",
      "31368/31368 [==============================] - 2454s 78ms/step - loss: 1.5383\n",
      "Epoch 3/20\n",
      "31368/31368 [==============================] - 2406s 77ms/step - loss: 1.5175\n",
      "Epoch 4/20\n",
      "31368/31368 [==============================] - 2402s 77ms/step - loss: 1.5066\n",
      "Epoch 5/20\n",
      "31368/31368 [==============================] - 2413s 77ms/step - loss: 1.4988\n",
      "Epoch 6/20\n",
      "31368/31368 [==============================] - 2417s 77ms/step - loss: 1.4936\n",
      "Epoch 7/20\n",
      "31368/31368 [==============================] - 35558s 1s/step - loss: 1.4896\n",
      "Epoch 8/20\n",
      "31368/31368 [==============================] - 2494s 79ms/step - loss: 1.4863\n",
      "Epoch 9/20\n",
      "31368/31368 [==============================] - 2570s 82ms/step - loss: 1.4838\n",
      "Epoch 10/20\n",
      "31368/31368 [==============================] - 278355s 9s/step - loss: 1.4810\n",
      "Epoch 11/20\n",
      "31368/31368 [==============================] - 2622s 84ms/step - loss: 1.4789\n",
      "Epoch 12/20\n",
      "31368/31368 [==============================] - 2602s 83ms/step - loss: 1.4769\n",
      "Epoch 13/20\n",
      "31368/31368 [==============================] - 12322s 393ms/step - loss: 1.4759\n",
      "Epoch 14/20\n",
      "31368/31368 [==============================] - 47499s 2s/step - loss: 1.4746\n",
      "Epoch 15/20\n",
      "31368/31368 [==============================] - 3717s 118ms/step - loss: 1.4732\n",
      "Epoch 16/20\n",
      "31368/31368 [==============================] - 3814s 122ms/step - loss: 1.4724\n",
      "Epoch 17/20\n",
      "31368/31368 [==============================] - 2877s 92ms/step - loss: 1.4717\n",
      "Epoch 18/20\n",
      "31368/31368 [==============================] - 8191s 261ms/step - loss: 1.4707\n",
      "Epoch 19/20\n",
      "31368/31368 [==============================] - 2885s 92ms/step - loss: 1.4698\n",
      "Epoch 20/20\n",
      "31368/31368 [==============================] - 4763s 152ms/step - loss: 1.4693\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
    "history = model.fit(dataset, epochs=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e0412ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This model predicts the next character written by Shakespeare. Preprocessing is required to feed text.\n",
    "\n",
    "def preprocess(texts):\n",
    "    X = np.array(tokenizer.texts_to_sequences(texts)) - 1\n",
    "    return tf.one_hot(X, max_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "aae3e9dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 882ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'u'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#test it out with a simple sentence\n",
    "\n",
    "X_new = preprocess(['How are yo'])\n",
    "Y_pred = np.argmax(model.predict(X_new), axis=-1)\n",
    "tokenizer.sequences_to_texts(Y_pred + 1)[0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4a6c2a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    X_new = preprocess([text])\n",
    "    Y_proba = model.predict(X_new)[0, -1:, :]\n",
    "    rescaled_logits = tf.math.log(Y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1) + 1\n",
    "    return tokenizer.sequences_to_texts(char_id.numpy())[0]\n",
    "\n",
    "def complete_text(text, n_chars=100, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ce420920",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 62ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 58ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 71ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 77ms/step\n",
      "1/1 [==============================] - 0s 67ms/step\n",
      "1/1 [==============================] - 0s 72ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 90ms/step\n",
      "1/1 [==============================] - 0s 68ms/step\n",
      "1/1 [==============================] - 0s 81ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "1/1 [==============================] - 0s 89ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 88ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 64ms/step\n",
      "1/1 [==============================] - 0s 79ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 93ms/step\n",
      "1/1 [==============================] - 0s 91ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 78ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 83ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 84ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 69ms/step\n",
      "1/1 [==============================] - 0s 87ms/step\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "1/1 [==============================] - 0s 80ms/step\n",
      "1/1 [==============================] - 0s 82ms/step\n",
      "1/1 [==============================] - 0s 85ms/step\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "1/1 [==============================] - 0s 70ms/step\n",
      "1/1 [==============================] - 0s 76ms/step\n",
      "1/1 [==============================] - 0s 86ms/step\n",
      "1/1 [==============================] - 0s 94ms/step\n",
      "Who are you profumed \n",
      "we laysing more sisters to his reason.\n",
      "\n",
      "katharina:\n",
      "well i never kath rival him my wife, i\n"
     ]
    }
   ],
   "source": [
    "#with these functions we can now print out some text.\n",
    "\n",
    "print(complete_text('Who are you', temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9eff577",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hardly King Lear, but we can see some characteristics of Shakespearean prose!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
